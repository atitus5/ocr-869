\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{pgfplots}

\usepackage{subcaption}

\usepackage{booktabs}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[breaklinks=true,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
%\pgfplotsset{compat=1.14}
\pgfplotsset{compat=1.10}

% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Error Correction for Optical Character Recognition via Language Models}

\author{Andrew Titus (\texttt{atitus}; 6.869), Brian Wheatman (\texttt{wheatman}; 6.869)\\
Massachusetts Institute of Technology\\
77 Massachusetts Avenue, Cambridge, MA 02139\\
{\tt\small \{atitus, wheatman\}@mit.edu}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Optical character recognition is the problem of recovering the text from an image.  This can be used in many contexts, including reading traffic signs for self-driving cars and digitizing historical documents.  In both of these cases, single character errors can lead to word errors and change the meaning of the text trying to be understood. It is also likely for these errors to arise regardless of the power of the system, due to uncontrollable external factors, such as views being obscured or degradation of the text itself. For this reason, we need a way to correct errors beyond the level of individual characters. We conclude that the usage of language models to perform higher-level error correction can be successful with simple optical character recognizers under certain noise conditions.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
\label{introduction}
Optical character recognition (OCR) is one of the oldest problems
in computer vision. The process of determining the characters
contained within an image has been researched since the
19\textsuperscript{th} century \cite{Eikvil93-OCR} long before the
development of computers, starting with C.R. Carey's retina
scanner system in 1870. With the rise of convolutional neural networks,
however, modern-day OCR systems now exceed human-level performance on
standard OCR tasks \cite{Chen15-BHR}.

If we have exceeded human-level performance for OCR systems, why should we
continue to care about the task? One reason is that deep neural networks
can be easily fooled -- sometimes, it only takes a change of one pixel
to cause a network to incorrectly classify an image \cite{Su17-OPA}.
If the change of a single pixel can potentially cause a neural
network-based OCR model to incorrectly classify a character, then
the system can potentially make character-level errors that propagate
into word-level errors that can fundamentally change the meaning of
a sentence or make it unintelligible. This motivates the need for
robust OCR systems that can recover any character-level errors made
by neural-based models. Moreover, with millions of parameters in modern
computer vision systems \cite{AlexNet, VGG}, it can be too memory-
and/or computation-intensive to run such models in mobile or low-power
settings. In such settings, it can be desirable to have a simpler model
whose errors can be recovered.

In this paper, we evaluate the feasibility of using language
models to recover errors made by modern OCR models.  We do this
by training a simple OCR model and then trying to improve the
accuracy by post-processing the result with character-level
bigram language models. The probabilities computed in these language
models are then used in graphical Markov models to correct errors by
determining the most likely character sequences.

The rest of this paper is organized as follows. Related work is
discussed in Section \ref{related_work}, followed by a
description of our experimental setup in Section \ref{exp_setup}.
We then introduce our methods for error correction in Section 
\ref{error_correction} and describe related results in
Section \ref{exp_results}. Finally, we conclude the paper and
discuss future work in Section \ref{conclusion_future_work}.

\subsection{Individual Contribution: AUTHOR NAME}
\label{individual_contribution}
TODO



\section{Related Work}
\label{related_work}

Interest in correcting errors in the output of OCR systems began as
early as 1975 \cite{Neuhoff75-TVA}, where English text is treated as
a Markov source and maximum \textit{a posteriori} (MAP) estimation
is performed via the Viterbi algorithm \cite{Forney73-TVA} determine
the most likely sequence of characters. Hanson \textit{et. al.}
showed in 1976 that binary $n$-grams (bigrams) can significantly
reduce the word error rate (WER) of text recognition systems
\cite{Hanson76-CIW}, whereas Pollock and Zamora showed in
1984 that dictionary-based spelling error correction systems
can recover $65-80\%$ of misspellings in scientific texts
\cite{Pollock84-ASC}. Additionally, Tong and Evans showed
that OCR systems can reduce their word error rate by $60.2\%$
by using the Viterbi algorithm with word-bigram probabilities, and
can even self-calibrate by using the learned confusion probabilities
for specific OCR environments \cite{Tong96-ASA}.

This prior research on lexical error correction has been successfully
implemented in more recent systems. Bissacco \textit{et. al.} from
Google developed an OCR system (``PhotoOCR'') in 2013 with record
performance on standard OCR tasks (such as Street View Text, ICDAR, etc.)
by combining deep neural network character classifiers with large,
distributed $n$-gram models \cite{Bissacco13-PRT}. Additionally, 
PhotoOCR was optimized for usage on mobile devices for memory and
latency, with a reported mean processing time of 600 milliseconds.



\section{Experimental Setup}
\label{exp_setup}

For text data, we chose the King James Version of the Bible, a
common text corpus in natural language processing. We then
generated $352 \times 672$ pixel images with 32 rows
of 32 characters arranged in a grid using 18 point Andal\'{e}
Mono monospace font. Due to the size of the King James Version of
the Bible (several million characters and therefore several thousand
images) and time constraints for the project, we decided to use a
subset of the dataset, the Book of Genesis, which resulted in 200 images
and approximately 200,000 character examples.

We used our local machine for training and evaluating models,
which had a NVIDIA GeForce GT 740M Graphics with 2048MB of memory.
All models and analysis code were developed in Python, with the
neural network-based models being implemented in PyTorch with CUDA
GPU support.



\section{Error Correction}
\label{error_correction}
As shown in Section \ref{related_work}, many OCR systems exhibit significant
decreases in both character error rate (CER) and word error rate (WER)
when language modeling techniques are applied to the outputs of an OCR
model. In this paper, we investigate a simple language model --
character-level bigrams -- and evaluate its performance in reducing CER
and WER with two different error correction methods.

\subsection{Character Bigram Language Model}

A character bigram is defined as two characters that are adjacent to
each other in a given text sequence. The generalization of a bigram,
the $n$-gram, is used in language modeling to determine conditional
probabilities for a token given the previous $(n - 1)$ tokens (the
formula for a bigram is given in Equation \ref{bigram_eq}).
Character-level $n$-grams are a logical choice for a language model
in an OCR error correction task because OCR systems fundamentally act
at the character level by classifying individual characters. However,
when compared to word-level $n$-grams, these probabilities are much
more localized and do not effectively represent word distribution,
sentence structure and other information that can be useful in correcting
word-level errors (and thus improving word error rate).

\begin{equation}
\label{bigram_eq}
P(W_n|W_{n-1}) = \frac{P(W_{n-1}, W_{n})}{P(W_{n-1})}
\end{equation}

We chose bigrams specifically because they are memory-efficient and
are easily incorporated into graphical Markov models. Formally, for
a text dataset containing $C$ unique characters, a bigram model
needs only $\Theta(C^2)$ space, whereas $n$-grams use
$\Theta(C^{n})$ space (although this can
be relatively small in practice for larger $n$ due to sparsity from
$n$-grams with no occurrences in the dataset). Additionally, $n$-grams
are used for the transition probabilities in
$(n-1)$\textsuperscript{th} order Markov models.
Higher-order Markov models are infrequently used due to their high
computational cost and necessity for specialized extensions of algorithms
for first-order Markov models \cite{DuPreez98-ETO}, so the choice of
$n$ for which the most efficient Markov model decoding techniques exist
is 2, which corresponds to bigrams.

\begin{figure}
\includegraphics[scale=.45]{24272959_889664494543135_1476429274_n.png}
\caption{Heatmap of character bigram probabilities}
\label{fig:bigram_heatmap}
\end{figure}

We also observed two interesting trends in the character bigram probabilities shown in Figure \ref{fig:bigram_heatmap}
that could be potentially exploited by the model during error correction.
The first is an empirical verification of Benford's Law (also known as
the Significant-Digit Law) \cite{Hill95-ASD}. The Significant Digit Law states
that in many real-life distributions of numbers, the digits 1--9 appear less
frequently as the digit grows larger, with the digit 1 appearing about
$30\%$ of the time. Indeed, we see such a distribution for the King James
Version of the Bible in the upper left corner of this heatmap! The
second observation is that there are three significant outliers in the
capital letter to capital letter section of the bigram probabilities:
$P(O | L)$, $P(R | O)$, and $P(D | R)$. This is due to the fact that every
mention of ``the Lord'' is stylized as ``the LORD'' in this version of the
Bible.

\subsection{Noise Model}
Our noise model simulates having a data set which has ``smudges'' (i.e.,
occasional blurry images) in it. For our training set, we use raw images and
then test on data that has been degraded by adding a random value to each
pixel. This value is drawn from a zero mean Gaussian distribution with
varying standard deviation ($\sigma$).  In Figure \ref{fig:e_images}, one can see
images of the letter ``e'' that has had noise added to it. One can see
that for low levels of noise (under $\sigma = 0.5$), a human can easily
determine the true character label; however, for higher levels of noise,
it can be difficult for even humans to determine the true character label.
To simulate smudging, we only add the noise to a random subset of the images.

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{10.png}
        \caption{$\sigma = 0.1$}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{30.png}
        \caption{$\sigma = 0.3$}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 

    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{60.png}
        \caption{$\sigma = 0.6$}
    \end{subfigure}
    ~ %
    \begin{subfigure}[b]{0.22\textwidth}
        \includegraphics[width=\textwidth]{100.png}
        \caption{$\sigma = 1.0$}
    \end{subfigure}
    \caption{\label{fig:e_images} Additive Gaussian error on the letter ``e'' with varying standard deviation $\sigma$}
\end{figure}

\subsection{Text as Probabilistic Graphical Model}

Next, we define the probabilistic graphical model that will be used
for the error correction methods (see Figure
\ref{fig:probabilistic_graphical_model}. Let $G = (V, E)$ be a directed
Markov chain with $V = \text{length of dataset in characters}$ hidden
states. The edge weights from vertex $i$ to $j$ are simply the
character-level bigram probabilities $P(j | i)$, whereas the emission
probabilities for vertex $i$ are simply the normalized probabilities 
$\Phi_i(:)$ output by the OCR model.

\begin{figure}
\includegraphics[scale=.8]{Capture.PNG}
\caption{\label{fig:probabilistic_graphical_model} Probabilistic graphical model for character-level error correction}
\end{figure}

\subsection{Belief Propagation}

One technique that we experimented with for correcting character-level
smudging errors is belief propagation. Belief propagation is a technique
that computes marginal probabilities for each state in a Markov chain
by passing ``messages'', which are simply partial sums from marginal
probability calculations of neighboring states that can be reused,
rather than recomputed. Formally, the marginal probability
of state $i$ being character $j$ is the product of incoming messages
$m_{i-1, i}$ and $m_{i + 1, i}$ from its neighbors $(i - 1)$ and
$(i + 1)$ with its own emission probability $\Phi_i(x_j)$, respectively, or:
\begin{equation}
\begin{split}
P_i(x_j) &= \Phi_i(x_j) * m_{i - 1, i}(x_j) * m_{i + 1, i}(x_j) \\
&= \Phi_i(x_j) * \left(\sum_{x_k} P(x_i | x_k) * m_{i - 2, i - 1}(x_k) \right) *\\
&\hspace{18mm} \left(\sum_{x_k} P(x_k | x_i) * m_{i + 1, i + 2}(x_k) \right) \\ 
\end{split}
\end{equation}

These messages are computed with a synchronous parallel update schedule,
where messages $m_{2, 3}(x_j), \cdots, m_{N - 1, N}(x_j)$ are computed in
the increasing direction with boundary condition
$m_{1, 2}(x_j) = \Phi_1(x_j) \sum_{x_k} P(x_k | x_j)$ at the same time that
messages $m_{N - 1, N - 2}(x_j), \cdots, m_{2, 1}(x_j)$ are computed in
the decreasing direction with boundary condition
$m_{N, N - 1}(x_j) = \Phi_N(x_j) \sum_{x_k} P(x_j | x_k)$.

The computational complexity of computing marginal probabilities for
$N$ nodes with $d$ possible states is thereby reduced from $O(d^N)$
(na\"{i}ve solution with recomputed messages) to $O(Nd^2)$ with
belief propagation (reusing messages). For OCR models, $d$ is
generally small (the number of unique characters) whereas $N$
can be on the order of millions of characters. Therefore, belief
propagation turns this intractable probability calculation into
something manageable on most computers.

\subsubsection{Backoff Alpha}

One thing that we noticed while running belief propagation on our OCR
results is that we had no control for how much messages for a given
state could affect messages for states that are many characters away.
Thus, we added a backoff component, $\alpha < 1.0$, to control roughly
at how many characters away $s$ the contribution of a given message would
become negligible (here, $<0.01$ times its original value). This modified
our formulas to the following, with only neglible increases in resulting
accuracy:

\begin{equation}
\alpha = 0.01^{1 / s}
\end{equation}

\begin{equation}
\begin{split}
P_i(x_j) &= \Phi_i(x_j) * m_{i - 1, i}(x_j) * m_{i + 1, i}(x_j) \\
&= \Phi_i(x_j) * \left(\sum_{x_k} P(x_i | x_k) * \alpha * m_{i - 2, i - 1}(x_k) \right) *\\
&\hspace{18mm} \left(\sum_{x_k} P(x_k | x_i) * \alpha * m_{i + 1, i + 2}(x_k) \right) \\ 
\end{split}
\end{equation}

\subsection{Viterbi Algorithm}

The Viterbi Algorithm, proposed in 1967 by Andrew Viterbi \cite{Forney73-TVA},
is a dynamic programming algorithm that finds the most likely
sequence of states in a 1\textsuperscript{st}-order hidden Markov model
with $N$ hidden states and $d$ possible values for each state.
The algorithm computes this sequence by maintaining two tables of
size $(d \times N)$: $T_1$, which stores the probability of the most likely
path seen so far; and $T_2$, which stores a ``back pointer'' to the previous
state in the most likely sequence so far. For our OCR Markov chain model,
we initialize $T_1[:,1] = \Phi_1(:)$ and $T_2[:,1] = 0$ (i.e., no back
pointer) and then perform updates for each node $i = 2, \cdots, N$
in order by iterating over each possible state $j = 1, \cdots, d$ and
updating tables as follows:
\begin{equation}
\begin{split}
T_1[j, i] &= \Phi_i(j) * \max_k \left( T_1[k, i - 1] * P(j | k) \right) \\
T_2[j, i] &= \arg \max_k \left( T_1[k, i - 1] * P(j | k) \right) \\
\end{split}
\end{equation}

Once complete, we can reassemble the most likely sequence by starting
at the most likely state for node $N$ and following the back pointers
in $T_2$ until we reach node 1, then reversing the sequence. The time
complexity of this algorithm is therefore also $O(Nd^2)$, so we have
a similar runtime to belief propagation.



\section{Experimental Results}
\label{exp_results}
We first noticed that in this constrained environment of only using the
same monospace font and not varying the text size that our simple,
1-layer linear model is able to get near-perfect character-level accuracy.
Furthermore, as one can see in Figure \ref{fig:CER}, the model is able to
determine the correct character under levels of noise that would make it
difficult even for humans. Even when $\sigma = 1.0$ and the image appears
to be almost completely unintelligible, the model is able to correctly
determine the character about 65\% of the time. 

\begin{figure}
\begin{tikzpicture}
\begin{axis}[
    xlabel={Additive Gaussian error $\sigma$},
    ylabel={Character Error Rate (CER; \%)},
    legend pos=north west
]
\addplot[
    color=black,
    mark=*,
    ]
    coordinates {
    (0,.661)(.1,.803)(.15,.892)(.2,.993)(.25,1.16)(.3,1.374)(.35,1.924)(.5,5.341)(.55, 7.319)(.6, 9.517)(.7, 15.237)(.75, 18.352)(.85, 24.817)(.9, 28.393)(.95, 31.44)(1, 34.662)
    };
\end{axis}
\end{tikzpicture}
\caption{\label{fig:CER} CER in the face of additive Gaussian noise}
\end{figure}

For each model that we trained, we calculate a confusion matrix. 
This is a matrix $M$ where $M[i][j]$ is the percentage of instances
in the data where $i$ was the true value and the model predicted $j$.
Figure \ref{fig:confusion} shows a confusion matrix for a model evaluated
on data with a relatively high amount of Gaussian noise ($\sigma = 0.6$)
added. We can still almost always determine the majority of the characters,
and the ones that are hard to determine are characters which appear very
few times (or not at all). Such characters are not learned very well due to
their lack of training examples and are unlikely to be guessed randomly,
even in the presence of high noise, as a result.

\begin{figure}
\includegraphics[scale=.6]{confusion.png}
\caption{\label{fig:confusion} Confusion matrix for adding Gaussian noise ($\sigma = 0.6$) to half of the images}
\end{figure}


\subsection{Error Correction}
Our results for fixing character-level errors in the noised images
were underwhelming.  We added noise to 20\%, 30\%,and 40\% of the
images with standard deviations between $\sigma = 0.4$ and $\sigma = 0.5$
since this (empirically) seemed to be the point at which it became difficult
for humans to determine the true labeling. This gave us CER between 3
and 7\%, and WER between 10 and 20\%.  After trying to fix it with both methods
(belief propagation and Viterbi algorithm), the CER went up and for belief
propagation, the WER also went up.  The Viterbi algorithm was able to
decrease the WER; however, it would only do so by about 0.1\%.
Overall, this method seems like it will work, but more complete
language models would be needed to significantly improve the performance.



\section{Conclusion and Future Work}
\label{conclusion_future_work}
While we did not observe as much of an improvement in CER and WER
as we would have hoped to see using these methods, we were indeed
able to improve the accuracy of OCR under certain conditions by
using language modeling. This could be further investigated in many
ways. The first is to use more elaborate language models, such as
larger character-level $n$-grams or using a word-level model.  We
could also perform experiments using a different base OCR model than just
a simple 1-layer linear classifier. It might even be possible to use a
smaller and/or simpler model than a fully-connected linear classifier.
Lastly, in a real-world setting, a user would hope to be able to use
this on a larger diversity of data, so we would expand our training
dataset to include different fonts, different text size and orientations,
and even possibly handwritten text.




{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}